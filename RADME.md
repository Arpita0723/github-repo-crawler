# ğŸ•¸ï¸ GitHub Repo Crawler

A Streamlit app to crawl any public GitHub repository and **download all `.py` files** recursively. Get a clean zipped copy of all the Python files in seconds!

---

## ğŸš€ Live App

ğŸ”— [Click here to use the app](https://yourname-yourrepo.streamlit.app)  
_(Replace with your actual Streamlit app link)_

---

## ğŸ“¦ Features

- ğŸ” Enter any public GitHub repo URL
- ğŸ Recursively finds all `.py` files (in folders/subfolders)
- ğŸ’¾ Saves the files locally
- ğŸ“¥ Provides CSV report of filenames
- ğŸ“¦ One-click ZIP download of all files

---

## ğŸ› ï¸ Technologies Used

- **Python**
- **Streamlit** â€“ for building the UI
- **Requests** â€“ for GitHub API calls
- **Pandas** â€“ for tabular reporting
- **Zipfile / OS** â€“ for file handling and compression

---

## ğŸ§ª How It Works

1. Paste the URL of a public GitHub repository
2. The app fetches the repo contents via the GitHub API
3. Recursively crawls through folders to find `.py` files
4. Saves them locally inside a `downloaded_files/` folder
5. Zips the folder and lets you download it

---

## ğŸ§° Installation (Run Locally)

```bash
git clone https://github.com/yourusername/github-repo-crawler.git
cd github-repo-crawler
python -m venv .venv
.\.venv\Scripts\activate   # On Windows
pip install -r requirements.txt
streamlit run app.py
